{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[4]\") \\\n",
    ".appName(\"Csv-Üzeri-SQL\") \\\n",
    ".config(\"spark.executor.memory\",\"4g\") \\\n",
    ".config(\"spark.driver.memory\",\"2g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".option(\"inferSchema\",\"True\") \\\n",
    ".option(\"sep\",\";\") \\\n",
    ".csv(\"D:\\\\Datasets\\\\OnlineRetail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2,55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2,75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|1.12.2010 08:26|     3,39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Örneğin yukarıda UnitPrice'ın  Float olmasına ihtiyaç var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark'ın çıkarımı bize her zaman yetmez ellerimizle kendi şemamızı yapalım\n",
    "Önce gerekli kütüphaneleri içeri alalım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType, DateType,DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elle şema oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_schema = StructType([\n",
    "    StructField(\"InvoiceNo\", StringType(), True),\n",
    "    StructField(\"StockCode\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"InvoiceDate\", StringType(), True),\n",
    "    StructField(\"UnitPrice\", FloatType(), True),\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Veriye elle hazırlanan şema ile tekrar okuma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".option(\"sep\",\";\") \\\n",
    ".schema(manual_schema) \\\n",
    ".csv(\"D:\\\\Datasets\\\\OnlineRetail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|     null|     null|       null|    null|       null|     null|      null|   null|\n",
      "|     null|     null|       null|    null|       null|     null|      null|   null|\n",
      "|     null|     null|       null|    null|       null|     null|      null|   null|\n",
      "|     null|     null|       null|    null|       null|     null|      null|   null|\n",
      "|     null|     null|       null|    null|       null|     null|      null|   null|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UnitPrice'da sıkıntı var. \".\" yerine \",\" olduğu için schma string'den float'a dönüştüremiyor. Çözüm bulmak gerekli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÇÖZÜM\n",
    "Çözüm olarak veriyi okurken \",\" ile \".\" yı değiştirerek okuyalım ve dataframe'i tekrar , ve . değişmiş\n",
    "şekilde diske yazalım. Diskten okurken düzeltilmiş csv den okuyalım ve elle hazırlanmış şemamızı kullanalım."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veriyi temizleyerek okuma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".option(\"inferSchema\",\"True\") \\\n",
    ".option(\"sep\",\";\") \\\n",
    ".csv(\"D:\\\\Datasets\\\\OnlineRetail.csv\") \\\n",
    ".withColumn(\"UnitPrice\",F.regexp_replace(F.col(\"UnitPrice\"), \",\",\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temiz dataframe'i diske yazalım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"sep\",\";\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .csv(\"D:\\\\Datasets\\\\OnlineRetail2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temizlenmiş olarak diske yazdığımız dosyayı tekrar okuyalım. Bu sefer manual_schema verelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temiz = spark.read \\\n",
    ".option(\"header\",\"True\") \\\n",
    ".schema(manual_schema) \\\n",
    ".option(\"sep\",\";\") \\\n",
    ".csv(\"D:\\\\Datasets\\\\OnlineRetail2\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|1.12.2010 08:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|1.12.2010 08:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|1.12.2010 08:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|1.12.2010 08:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|1.12.2010 08:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temiz.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
